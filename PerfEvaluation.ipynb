{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rdkit import Chem, RDLogger\n",
    "\n",
    "# Shut up RDKit\n",
    "logger = RDLogger.logger()\n",
    "logger.setLevel(RDLogger.CRITICAL)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "import random\n",
    "\n",
    "import importlib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./SD_LSTM/')\n",
    "sys.path.append('./LSTM_TF/')\n",
    "sys.path.append('./VANILLA_VAE/')\n",
    "sys.path.append('./SD_VAE/')\n",
    "\n",
    "from sd_vae_sampler import SDVAESampler\n",
    "from model_sd_vae import SDVAE\n",
    "from sd_lstm_sampler import SDLSTMSampler\n",
    "from sd_lstm_utils import load_model as load_sd_lstm_model\n",
    "\n",
    "from benchmark_vanilla_vae import VanillaVAEHarness\n",
    "\n",
    "from model_vanilla_vae import VanillaMolVAE\n",
    "from rnn_utils import load_model\n",
    "\n",
    "from fast_rnn_sampler import FastSampler\n",
    "# from rnn_sampler import ConditionalSmilesRnnSampler\n",
    "from rnn_utils import load_rnn_model\n",
    "\n",
    "\n",
    "sys.path.append('utils/')\n",
    "\n",
    "from smiles_char_dict import SmilesCharDictionary\n",
    "sd = SmilesCharDictionary()\n",
    "\n",
    "from evaluation_utils import absolute_metrics, amina_metrics, property_metrics, props_from_smiles, plot_smiles, benchmark_reconstruction_QM9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../utils/')\n",
    "from property_calculator import PropertyCalculator\n",
    "\n",
    "pc = PropertyCalculator(['LogP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXP-cVAE-Pol   EXP-cVAE-TF10\t Old-cVAE-Pol  cVAE-Pol\n",
      "EXP-cVAE-Pol2  Old-Neg-cVAE-Pol  cVAE-KLD      cVAE-Pol2\n"
     ]
    }
   ],
   "source": [
    "!ls models/NEW_LONG_RUNS/QM9/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a Conv1d inited\n",
      "a Conv1d inited\n",
      "a Conv1d inited\n",
      "a Linear inited\n",
      "a Linear inited\n",
      "a Linear inited\n",
      "a Linear inited\n",
      "a GRU inited\n",
      "a Linear inited\n"
     ]
    }
   ],
   "source": [
    "# Load Vanilla cVAE Model\n",
    "\n",
    "# model_definit = 'models/LONG_RUNS/QM9/cVAE/SD_REG_VANILLA_VAE_quiet-dew-12_Epoch_420_Vl_0.115.json'\n",
    "# model_weights = 'models/LONG_RUNS/QM9/cVAE/SD_REG_VANILLA_VAE_quiet-dew-12_Epoch_420_Vl_0.115.pt'\n",
    "\n",
    "# model_definit = 'models/NEW_LONG_RUNS/QM9/cVAE-KLD/SD_REG_VANILLA_VAE_ancient-paper-11_Epoch_235_Vl_0.353.json'\n",
    "# model_weights = 'models/NEW_LONG_RUNS/QM9/cVAE-KLD/SD_REG_VANILLA_VAE_ancient-paper-11_Epoch_235_Vl_0.353.pt'\n",
    "\n",
    "# model_definit = 'models/NEW_LONG_RUNS/QM9/Old-cVAE-Pol/SD_REG_VANILLA_VAE_delicate-queen-93_Epoch_195_Vl_0.043.json'\n",
    "# model_weights = 'models/NEW_LONG_RUNS/QM9/Old-cVAE-Pol/SD_REG_VANILLA_VAE_delicate-queen-93_Epoch_195_Vl_0.043.pt'\n",
    "\n",
    "# model_definit = 'models/NEW_LONG_RUNS/QM9/Old-Neg-cVAE-Pol/SD_REG_VANILLA_VAE_ancient-tooth-26_Epoch_190_Vl_0.713.json'\n",
    "# model_weights = 'models/NEW_LONG_RUNS/QM9/Old-Neg-cVAE-Pol/SD_REG_VANILLA_VAE_ancient-tooth-26_Epoch_190_Vl_0.713.pt'\n",
    "\n",
    "# model_definit = 'models/NEW_LONG_RUNS/QM9/cVAE-Pol/SD_REG_VANILLA_VAE_young-king-99_Epoch_381_Vl_0.165.json'\n",
    "# model_weights = 'models/NEW_LONG_RUNS/QM9/cVAE-Pol/SD_REG_VANILLA_VAE_young-king-99_Epoch_381_Vl_0.165.pt'\n",
    "\n",
    "# model_definit = 'models/NEW_LONG_RUNS/QM9/cVAE-Pol/SD_REG_VANILLA_VAE_young-king-99_Epoch_50_Vl_0.262.json'\n",
    "# model_weights = 'models/NEW_LONG_RUNS/QM9/cVAE-Pol/SD_REG_VANILLA_VAE_young-king-99_Epoch_50_Vl_0.262.pt'\n",
    "\n",
    "# model_definit = 'models/NEW_LONG_RUNS/QM9/cVAE-Pol2/SD_REG_VANILLA_VAE_still-rain-34_Epoch_313_Vl_0.349.json'\n",
    "# model_weights = 'models/NEW_LONG_RUNS/QM9/cVAE-Pol2/SD_REG_VANILLA_VAE_still-rain-34_Epoch_313_Vl_0.349.pt'\n",
    "\n",
    "# model_definit = 'models/NEW_LONG_RUNS/QM9/Weighted-cVAE-Pol/SD_REG_VANILLA_VAE_lingering-sunset-29_Epoch_78_Vl_0.096.json'\n",
    "# model_weights = 'models/NEW_LONG_RUNS/QM9/Weighted-cVAE-Pol/SD_REG_VANILLA_VAE_lingering-sunset-29_Epoch_78_Vl_0.096.pt'\n",
    "\n",
    "# model_definit = 'models/NEW_LONG_RUNS/QM9/Weighted-cVAE-Pol/SD_REG_VANILLA_VAE_lingering-sunset-29_Epoch_264_Vl_0.053.json'\n",
    "# model_weights = 'models/NEW_LONG_RUNS/QM9/Weighted-cVAE-Pol/SD_REG_VANILLA_VAE_lingering-sunset-29_Epoch_264_Vl_0.053.pt'\n",
    "\n",
    "# model_definit = 'models/NEW_LONG_RUNS/QM9/Weighted-cVAE-Pol2/SD_REG_VANILLA_VAE_red-waterfall-24_Epoch_300_Vl_0.076.json'\n",
    "# model_weights = 'models/NEW_LONG_RUNS/QM9/Weighted-cVAE-Pol2/SD_REG_VANILLA_VAE_red-waterfall-24_Epoch_300_Vl_0.076.pt'\n",
    "\n",
    "model_definit = 'models/NEW_LONG_RUNS/QM9/cVAE/SD_REG_VANILLA_VAE_dawn-band-89_Epoch_294_Vl_0.086.json'\n",
    "model_weights = 'models/NEW_LONG_RUNS/QM9/cVAE/SD_REG_VANILLA_VAE_dawn-band-89_Epoch_294_Vl_0.086.pt'\n",
    "\n",
    "cvae_sampler = VanillaVAEHarness(batch_size=64, device='cpu')\n",
    "\n",
    "cvae_model = load_model(model_class=VanillaMolVAE, model_definition=model_definit, model_weights=model_weights, device='cpu')\n",
    "\n",
    "# ugly hack\n",
    "cvae_model = cvae_model.to('cpu')\n",
    "cvae_model.device = 'cpu'\n",
    "cvae_model.encoder.device = 'cpu'\n",
    "cvae_model.state_decoder.device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a Conv1d inited\n",
      "a Conv1d inited\n",
      "a Conv1d inited\n",
      "a Linear inited\n",
      "a Linear inited\n",
      "a Linear inited\n",
      "a GRU inited\n",
      "a Linear inited\n"
     ]
    }
   ],
   "source": [
    "# Load Explicit cVAE Model\n",
    "\n",
    "# TODO: FIX NAME CLASHES\n",
    "file_path = 'ACTION_SAMPLING_VANILLA_VAE/model_vanilla_vae.py'\n",
    "\n",
    "# Load the module specified by the file path\n",
    "spec = importlib.util.spec_from_file_location(\"model_vanilla_vae.py\", file_path)\n",
    "module = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"model_vanilla_vae\"] = module\n",
    "spec.loader.exec_module(module)\n",
    "\n",
    "explicit_model_class = module.VanillaMolVAE\n",
    "\n",
    "file_path = 'ACTION_SAMPLING_VANILLA_VAE/action_sampling_vae_sampler.py'\n",
    "\n",
    "# Load the module specified by the file path\n",
    "spec = importlib.util.spec_from_file_location(\"action_sampling_vae_sampler.py\", file_path)\n",
    "module = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"action_sampling_vae_sampler\"] = module\n",
    "spec.loader.exec_module(module)\n",
    "\n",
    "harness_class = module.VanillaVAEHarness\n",
    "\n",
    "# model_definit = 'models/NEW_LONG_RUNS/QM9/EXP-cVAE-Pol2/SD_REG_VANILLA_VAE_shiny-sun-14_Epoch_170_Vl_0.244.json'\n",
    "# model_weights = 'models/NEW_LONG_RUNS/QM9/EXP-cVAE-Pol2/SD_REG_VANILLA_VAE_shiny-sun-14_Epoch_170_Vl_0.244.pt'\n",
    "\n",
    "# model_definit = 'models/NEW_LONG_RUNS/QM9/EXP-cVAE-Pol/SD_REG_VANILLA_VAE_fragrant-math-98_Epoch_127_Vl_0.367.json'\n",
    "# model_weights = 'models/NEW_LONG_RUNS/QM9/EXP-cVAE-Pol/SD_REG_VANILLA_VAE_fragrant-math-98_Epoch_127_Vl_0.367.pt'\n",
    "\n",
    "model_definit = 'models/NEW_LONG_RUNS/QM9/EXP-cVAE-Pol2-TF10/SD_REG_VANILLA_VAE_weathered-meadow-69_Epoch_143_Vl_0.126.json'\n",
    "model_weights = 'models/NEW_LONG_RUNS/QM9/EXP-cVAE-Pol2-TF10/SD_REG_VANILLA_VAE_weathered-meadow-69_Epoch_143_Vl_0.126.pt'\n",
    "\n",
    "exp_cvae_sampler = harness_class(batch_size=64, device='cpu')\n",
    "\n",
    "exp_cvae_model = load_model(model_class=explicit_model_class, model_definition=model_definit, model_weights=model_weights, device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrome/Code/MolGen/my_code/ACTION_SAMPLING_VANILLA_VAE/model_vanilla_vae.py:315: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CCCCCCCCCCCCCCC((((OO))', 'CCCCCCCCCCCCCC((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((', 'CCCCCCCCCCCCCCCCCCCCCC111111111111111111111111111111111111111111111111111111111111111111111111111111', 'CCCCCCCCCCCCCCCCCCNNNN1', 'OOcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc', 'CCCCCCCCCCCCCCCCCCCCCC', 'OOcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc', 'CCCCCCCCCCCCCCCCCCNNNN1', 'OOnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc', 'CCCCCCCCCCCCCCCCCCNNNN1', 'CCCCCCCCCCCCCCCCCCCCCN', 'CCCCCCCCCCCC))O4444444444444444444444444444444444444444444444444444444444444444444444444444444444444', 'OOcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc', 'OOcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccnnnnnnn', 'CCCCCCCCCCCCCCCCCCCNNNN', 'OOcccccccccccccccccccccooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooccccccccc', 'NNNNNNNNNNNNNNO', 'CCCCCCCCCCCC11((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((', 'CCCCCCCCCCCCCCCCCCN1111', 'CCCCCCCCCCCCCCCCCCC11NNN11', 'CCCCCCCCCCCCCCCCCCCC1NNN', 'OOcccccccccccoooooooooooooooooooo]]ccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc', 'CCCCCCCCCCCCCCCCCCC1NN1', 'CCCCCCCCCCCCC11OO', 'CCCCCCCCCCCCC1((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((', 'CCCCCCCCCCCCC11', 'CCCCCCCCCCCCCCCCCCC11NNNNNNN11', 'CCCCCCCCCCCCCCCCCCN1111', 'CCCCCCCCCCCCC(((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((', 'OOOOOOOOOOOOOOOO', 'CCCCCCCCCCCCCCNN11nnnnnnn111111111111111111111111111111111111111111111111111111111111111111111111111', 'CCCCCCCCCCCCCCCCCCCCCCCC11NNNNN11', 'OOccccccccooooooooooooooo]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]cccccccccccccccccccccccccccccccccccccc', 'CCCCCCCCCCCCCCCCCCC14OO', 'NN-CCCCCCCCCCCCCCCC(((((((((((((((((((((((((((((4444444444444444444444444444444444444444444444444444', 'NNNNNNNNNNNNNNO', 'OOcccccccccccccccccccccccccccccccccccnnnnnnnnnnnnnnoooooooooooooooooooo---------', 'NNcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc', 'CCCCCCCCCCCCCCC(((((OOOOOOOOOOOOOOOOOOOO', 'NNNNNNNNNNNNNNNNN', 'cccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc', 'OOcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc', 'OOcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc', 'CCCCCCCCCCCC11((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((', 'CCCCCCCCCCCCCCC(((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((', 'CCCCCCCCCCCCCCCCCC1444', 'NN--]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]', 'CCCCCCCCCCCCC333333333333333333333333333333333333333333333333333333333333333333333333333333333333333', 'cccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc', 'CCCCCCCCCCCCCCCCCCC11NNNNNN']\n"
     ]
    }
   ],
   "source": [
    "# sample(self, model, properties, random=True, latent_points = None)\n",
    "# sd.decode()\n",
    "sample_props = np.array( [[a] for a in random.choices(test_props, k=50)])\n",
    "print(list(exp_cvae_sampler.sample(exp_cvae_model, sample_props)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ZINC_ID', 'SMILES', 'LogP'], dtype='object')\n",
      "Properties in dataset: ['LogP']\n"
     ]
    }
   ],
   "source": [
    "## ZINC\n",
    "dataset_path = \"data/ZINC250K/\"\n",
    "qmds = pd.read_csv(os.path.join(dataset_path, 'ZINC_clean.csv'))\n",
    "print(qmds.columns)\n",
    "props = list(qmds.drop(['ZINC_ID', 'SMILES'], axis=1).columns)\n",
    "print(f'Properties in dataset: {props}')\n",
    "\n",
    "# Seperate Test Train and Validation Datasets\n",
    "indecies = np.load('data/ZINC250K/data_splits.npy')\n",
    "\n",
    "train_smiles = np.array(qmds['SMILES'])[indecies == 0]\n",
    "val_smiles = np.array(qmds['SMILES'])[indecies == 1]\n",
    "test_smiles = np.array(qmds['SMILES'])[indecies == 2]\n",
    "\n",
    "test_props = [ float(a) for a in pd.read_csv('data/ZINC250K/ZINC_clean.csv')['LogP'][indecies == 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['QM9_id', 'SMILES', 'LogP'], dtype='object')\n",
      "Properties in dataset: ['LogP']\n"
     ]
    }
   ],
   "source": [
    "## QM9\n",
    "dataset_path = \"data/QM9/\"\n",
    "qmds = pd.read_csv(os.path.join(dataset_path, 'QM9_clean.csv'))\n",
    "print(qmds.columns)\n",
    "props = list(qmds.drop(['QM9_id', 'SMILES'], axis=1).columns)\n",
    "print(f'Properties in dataset: {props}')\n",
    "\n",
    "# Seperate Test Train and Validation Datasets\n",
    "indecies = np.load('data/QM9/data_splits.npy')\n",
    "\n",
    "train_smiles = np.array(qmds['SMILES'])[indecies == 0]\n",
    "val_smiles = np.array(qmds['SMILES'])[indecies == 1]\n",
    "test_smiles = np.array(qmds['SMILES'])[indecies == 2]\n",
    "\n",
    "test_props = [ float(a) for a in pd.read_csv('data/QM9/QM9_clean.csv')['LogP'][indecies == 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Train Unconditional Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prior_metrics_conditional_model(model, sampler, train_set, test_props, num_sample = 1000, num_decode = 100):\n",
    "    '''\n",
    "    We sample 1000 latent representations z ∼ N (O, I). For each of them we decode 100 times, \n",
    "    and calculate the portion of 100,000 decoded results that corresponds to valid Program or SMILES sequences.\n",
    "\n",
    "    %Valid = #Valid / #Generations\n",
    "    %Unique = #Unique Valid / #Valid\n",
    "    %Novel = #Valid - #Valid_in_train_set / #Valid\n",
    "    MSE = Mse (target_property - achieved_property)^2\n",
    "    '''\n",
    "\n",
    "    # TODO: Currently hardcoded for ['LogP']\n",
    "    \n",
    "    # Check we have enough condit. values to give to model\n",
    "    assert num_sample < len(test_props)\n",
    "    train_set = set(train_set.copy())\n",
    "    \n",
    "    test_props = test_props.copy()\n",
    "\n",
    "    # Get num_sample properties\n",
    "    props = random.sample(test_props, num_sample)\n",
    "\n",
    "    n_all_smiles = 0\n",
    "    n_valid_smiles = 0\n",
    "    n_valid_in_train = 0\n",
    "    \n",
    "    unique_valid = set()\n",
    "\n",
    "    total_se = 0\n",
    "\n",
    "    for prop in tqdm(props):\n",
    "        repeated_prop = torch.tensor([[prop]] * num_decode)\n",
    "        prop_smiles =  sampler.sample(model, repeated_prop.clone())\n",
    "\n",
    "        for smi in prop_smiles:\n",
    "            n_all_smiles += 1\n",
    "            # Check if valid\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "\n",
    "            batch_mse = 0\n",
    "\n",
    "            if mol is not None: # mol is valid\n",
    "                can_smi = Chem.MolToSmiles(mol, canonical=True)\n",
    "                n_valid_smiles += 1\n",
    "                unique_valid.add(can_smi)\n",
    "\n",
    "                # Compute property\n",
    "                total_se += ( pc(mol)[0] - prop)**2\n",
    "                          \n",
    "                # Check if in train set\n",
    "                if can_smi in train_set:\n",
    "                    n_valid_in_train += 1\n",
    "\n",
    "    # Compute metrics\n",
    "    validity = n_valid_smiles / n_all_smiles\n",
    "    uniqueness = len(unique_valid) / n_valid_smiles\n",
    "    novelty = (n_valid_smiles - n_valid_in_train) / n_valid_smiles\n",
    "    mse = total_se / n_valid_smiles\n",
    "\n",
    "    assert n_all_smiles == num_sample * num_decode\n",
    "\n",
    "    return {'validity' : validity,\n",
    "            'uniqueness' : uniqueness,\n",
    "            'novelty' : novelty,\n",
    "            'MSE': mse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_tokens(test_smiles, max_seq_len):\n",
    "    \n",
    "    tokens = torch.zeros([len(test_smiles), max_seq_len], dtype=torch.long)\n",
    "    \n",
    "    for i in range(len(test_smiles)):\n",
    "        smi = sd.encode(test_smiles[i])\n",
    "        tokens[i][0] = sd.char_idx[sd.BEGIN]\n",
    "\n",
    "        for j in range(len(smi)):\n",
    "            tokens[i][j+1] = sd.char_idx[smi[j]]\n",
    "            \n",
    "        tokens[i][len(smi)+1] = sd.char_idx[sd.END]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def compute_reconstruction_vae_model(test_smiles, test_props, vae_model, vae_sampler, n_encodes = 10, n_decodes = 25):\n",
    "    '''\n",
    "    test_smiles is array-like of test smiles for reconstruction\n",
    "    test_props is \n",
    "\n",
    "    n_encodes is number of encodings per test_set instance\n",
    "    n_decodes is number of decodings per latent encoding\n",
    "\n",
    "    Do one at a time since we're deconding 250x more points\n",
    "\n",
    "    From SD-VAE paper:\n",
    "    for each of the structured data in the held-out dataset, we encode it 10 times and decoded (for each encoded latent space representation) 25 times, and report the portion of decoded structures that are the same as the input ones\n",
    "    '''\n",
    "    n_same = 0\n",
    "    n_total = 0\n",
    "    \n",
    "    test_props = test_props.copy()\n",
    "        \n",
    "    # Normalize properties\n",
    "    normd_props = torch.tensor([[a] for a in test_props])\n",
    "\n",
    "    normd_props = vae_model.normalize_prop_scores(normd_props)\n",
    "\n",
    "    vae_model.eval()\n",
    "    vae_model.reparam = True\n",
    "\n",
    "\n",
    "    for smindex in tqdm(range(len(test_smiles))):\n",
    "        # max_seq_len = model.max_decode_steps\n",
    "        # enc_b_size = n_encodes\n",
    "        smile = test_smiles[smindex:smindex+1]\n",
    "        prop = normd_props[smindex:smindex+1]\n",
    "\n",
    "        # Convert smiles to tokens\n",
    "        tokens = smiles_to_tokens(smile, vae_model.max_decode_steps)\n",
    "\n",
    "        input_tokens = tokens.repeat(n_encodes, 1)\n",
    "        input_properties = prop.repeat(n_encodes, 1)\n",
    "        \n",
    "        # Run through the model\n",
    "        encodings = vae_model.encoder(input_tokens)[0]\n",
    "        samp_latents = vae_model.reparameterize(mu = encodings, logvar = torch.tensor(vae_model.eps_std))\n",
    "\n",
    "        # Repeat latents n_decode times\n",
    "        # Repeat properties n_decode times\n",
    "        samp_latents = samp_latents.repeat(n_decodes, 1) \n",
    "        repeated_props = input_properties.repeat(n_decodes, 1)\n",
    "\n",
    "        sampled_smiles = vae_sampler.sample(model=vae_model, properties=repeated_props, random = True, latent_points = samp_latents)\n",
    "\n",
    "        # Samp latents now contains n_encodings encoded (random latent points)\n",
    "        # TODO: Do I reparam here? It's the only source of randomness for encodings but not how inference with a vae is typically done\n",
    "        # feels right for judging the reconstruction loss though\n",
    "\n",
    "        # Now decode probabilitically n_decode times\n",
    "        counts = Counter(sampled_smiles)\n",
    "\n",
    "        n_same += counts[smile[0]]\n",
    "\n",
    "        n_total += len(sampled_smiles)\n",
    "\n",
    "    return n_same / n_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cvae_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/chrome/Code/MolGen/my_code/PerfEvaluation.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/chrome/Code/MolGen/my_code/PerfEvaluation.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m compute_prior_metrics_conditional_model(cvae_model, cvae_sampler, train_smiles, test_props \u001b[39m=\u001b[39m test_props, num_sample \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m, num_decode \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cvae_model' is not defined"
     ]
    }
   ],
   "source": [
    "compute_prior_metrics_conditional_model(cvae_model, cvae_sampler, train_smiles, test_props = test_props, num_sample = 200, num_decode = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrome/Code/MolGen/my_code/ACTION_SAMPLING_VANILLA_VAE/action_sampling_vae_sampler.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  properties = torch.tensor(properties).clone()\n",
      "100%|██████████| 200/200 [00:56<00:00,  3.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'validity': 0.161,\n",
       " 'uniqueness': 0.38509316770186336,\n",
       " 'novelty': 1.0,\n",
       " 'MSE': 33.58561360237206}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_prior_metrics_conditional_model(exp_cvae_model, exp_cvae_sampler, train_smiles,  test_props = test_props, num_sample = 200, num_decode = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1000\n",
    "smiles_1k = random.choices(test_smiles, k=500) # 100\n",
    "properties_1k = random.choices(test_props, k=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]/home/chrome/Code/MolGen/my_code/./VANILLA_VAE/benchmark_vanilla_vae.py:128: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  latent_points = torch.tensor(latent_points, dtype=torch.float32)\n",
      "100%|██████████| 500/500 [01:40<00:00,  4.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.62096"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_reconstruction_vae_model(smiles_1k, properties_1k, cvae_model, cvae_sampler, n_encodes = 5, n_decodes = 5) #10 , 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'exp_cvae_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/chrome/Code/MolGen/my_code/PerfEvaluation.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/chrome/Code/MolGen/my_code/PerfEvaluation.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m compute_reconstruction_vae_model(smiles_1k, properties_1k, exp_cvae_model, exp_cvae_sampler, n_encodes \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m, n_decodes \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m) \u001b[39m#10 , 25\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exp_cvae_model' is not defined"
     ]
    }
   ],
   "source": [
    "compute_reconstruction_vae_model(smiles_1k, properties_1k, exp_cvae_model, exp_cvae_sampler, n_encodes = 5, n_decodes = 5) #10 , 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SD-LSTM Model\n",
    "lstm_params = 'models/LONG_RUNS/ZINC/LSTM/LSTM_final_0.769.pt'\n",
    "lstm_definit = 'models/LONG_RUNS/ZINC/LSTM/LSTM_final_0.769.json'\n",
    "\n",
    "lstm_sampler = FastSampler(device = 'cpu', batch_size=64)\n",
    "lstm_model = load_rnn_model(\n",
    "            model_definition= lstm_definit,\n",
    "            model_weights = lstm_params,\n",
    "            device = 'cpu',\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_lstm(lstm_sampler, lstm_model, target_props, train_smiles, seq_len = 278):\n",
    "    # Sample smiles from property scores\n",
    "    sample_smiles = lstm_sampler.sample(model=lstm_model, properties=target_props, num_to_sample=len(target_props), max_seq_len=seq_len)\n",
    "    assert len(sample_smiles) == len(target_props)\n",
    "    n_decodes = 0\n",
    "    n_valid = 0\n",
    "    n_valid_in_train = 0\n",
    "    unique_valid = set()\n",
    "    total_se = 0.0\n",
    "    train_set = set(train_smiles)\n",
    "\n",
    "    for i in range(len(sample_smiles)):\n",
    "        n_decodes += 1\n",
    "        smi = sample_smiles[i]\n",
    "        prop = target_props[i]\n",
    "\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "\n",
    "        if mol is not None:\n",
    "            n_valid += 1\n",
    "            can_smi = Chem.MolToSmiles(mol, canonical=True)\n",
    "            unique_valid.add(can_smi)\n",
    "            \n",
    "            total_se += ( pc(mol)[0] - prop)**2\n",
    "                          \n",
    "        # Check if in train set\n",
    "        if can_smi in train_set:\n",
    "            n_valid_in_train += 1\n",
    "\n",
    "    validity = n_valid / n_decodes\n",
    "    uniqueness = len(unique_valid) / n_valid\n",
    "    novelty = (n_valid - n_valid_in_train) / n_valid\n",
    "    mse = total_se / n_valid\n",
    "\n",
    "\n",
    "    return validity, uniqueness, novelty, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_props = random.choices(test_props, k=10000)\n",
    "target_props = torch.tensor([[a] for a in target_props])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics_lstm(lstm_sampler, lstm_model, train_smiles=train_smiles, target_props=target_props)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Molecule Generation Venv",
   "language": "python",
   "name": "molvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
